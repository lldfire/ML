{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 第5章 决策树\n",
    "1. 决策树是基于特征对实例进行分类的树形结构。\n",
    "2. 决策树学习旨在构建一个与训练数据拟合好，并且复杂度小的决策树\n",
    "3. 决策树算法包括三部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、C4.5、CART。\n",
    "4. 特征选择的目的在于选择对训练数据能正确分类的特征，常用的准则如下：\n",
    "    - 样本集合D对特征A的信息增益（ID3）\n",
    "    $$H(D|A) = \\sum_{i=1}^n \\frac{|D_i|}{D} H(D_i)$$\n",
    "    $H(D)$是数据集D的熵，$H(D_i)$是数据集$D_i$的熵，$D_i$是数据集D中特征A取第 i 个值的样本子集，\n",
    "    - 样本集合D对特征A的信息增益率（C4.5）\n",
    "    - 样本集合D的基尼系数（CART）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "def createdata():\n",
    "    datasets = np.array([\n",
    "        ['青年', '否', '否', '一般', '否'],\n",
    "        ['青年', '否', '否', '好', '否'],\n",
    "        ['青年', '是', '否', '好', '是'],\n",
    "        ['青年', '是', '是', '一般', '是'],\n",
    "        ['青年', '否', '否', '一般', '否'],\n",
    "        ['中年', '否', '否', '一般', '否'],\n",
    "        ['中年', '否', '否', '好', '否'],\n",
    "        ['中年', '是', '是', '好', '是'],\n",
    "        ['中年', '否', '是', '非常好', '是'],\n",
    "        ['中年', '否', '是', '非常好', '是'],\n",
    "        ['老年', '否', '是', '非常好', '是'],\n",
    "        ['老年', '否', '是', '好', '是'],\n",
    "        ['老年', '是', '否', '好', '是'],\n",
    "        ['老年', '是', '否', '非常好', '是'],\n",
    "        ['老年', '否', '否', '一般', '否'],\n",
    "    ])\n",
    "\n",
    "    feature = np.array([u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别'])\n",
    "\n",
    "    return {'data': datasets, 'feature': feature}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益\n",
    "- 定义：特征A对训练数据集D的信心增益$g(D, A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下的经验条件熵$H(D|A)$\n",
    "- 理解：经验熵$H(D)$表示对数据集D分类的不确定性，经验条件熵$H(D|A)$表示在特征A给定的条件下对数据集D进行分类的不确定性。他们的差，即信息增益就表示由于特征A使得对数据集D进行分类的不确定性减少的程度。\n",
    "- 结论：信息增益大的特征具有更强的分类能力。\n",
    "### 算法\n",
    "- ：数据集D和特征A\n",
    "- ：特征A对训练数据集D的信息增益$g(D, A)$\n",
    "1. 计算数据集D经验熵$H(D)$\n",
    "$$H(D)=-\\sum \\frac{|C_k|}{|D|}\\log_2 \\frac{|C_k|}{|D|}$$\n",
    "    - $|C_k|$属于类$C_k$的个数\n",
    "    - $|D|$ 样本容量，即样本个数\n",
    "2. 计算特征A对数据集D的经验条件熵$H(D|A)$\n",
    "$$H(D|A) = \\sum_{i=1}^n \\frac{|D_i|}{D} H(D_i) = -$$\n",
    "\n",
    "3. 计算信息增益\n",
    "$$g(D, A) = H(D) - H(D|A)$$\n",
    "\n",
    "### 延伸\n",
    "数据集的经验熵相同，所以条件经验熵越小，信息增益越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算经验熵,\n",
    "def empirical_entropy(data):\n",
    "    \"\"\" 计算数据集的经验熵 \"\"\"\n",
    "    data_len = data.shape[0]\n",
    "    labels_count = Counter(data[:, -1])    # 统计类别结果\n",
    "    ent = -sum([\n",
    "        (c / data_len) * np.log2(c / data_len) for c in labels_count.values()])\n",
    "    return ent\n",
    "\n",
    "\n",
    "# 计算条件经验熵\n",
    "def cond_empirical_entropy(dataset, axis=0):\n",
    "    \"\"\" 计算特征对数据集的条件经验熵 \n",
    "    datasets: array, list\n",
    "    \"\"\"\n",
    "    data_length = dataset.shape[0]\n",
    "    # 某个特征取值将数据集分为与特征取值数量相同的类别\n",
    "    # 统计数据集中某一列\n",
    "    feature_count = Counter(dataset[:, axis])\n",
    "    # 计算每个特征取值对应的数据集的经验熵\n",
    "    cond_ent = []\n",
    "    for feature in feature_count:\n",
    "        D_i_D = feature_count[feature] / data_length\n",
    "        print(D_i_D)\n",
    "        indexs = dataset[:, axis] == feature\n",
    "        ent = empirical_entropy(dataset[indexs])\n",
    "        print(ent)\n",
    "        cond_ent.append(ent * D_i_D)\n",
    "    \n",
    "    return sum(cond_ent)\n",
    "\n",
    "# 信息增益\n",
    "def info_gain(ent, cond_ent):\n",
    "    return ent - cond_ent\n",
    "\n",
    "# 查看数据每个特征的信息增益\n",
    "# ent = empirical_entropy(xindai['data'])\n",
    "# for i in range(4):\n",
    "#     cond_ent = cond_empirical_entropy(xindai['data'], i)\n",
    "#     \n",
    "    \n",
    "# 定义函数，计算信息增益\n",
    "def train_info_gain(dataset):\n",
    "    count = dataset['data'].shape[1] - 1\n",
    "    ent = empirical_entropy(dataset['data'])   # 计算数据集的经验熵\n",
    "    \n",
    "    features = []\n",
    "    for c in range(count):\n",
    "        cond_ent = cond_empirical_entropy(dataset['data'], c)\n",
    "        c_info_gain = round(info_gain(ent, cond_ent), 2)\n",
    "        features.append((c, c_info_gain))\n",
    "        \n",
    "        print(f'当前特征：{dataset[\"feature\"][c]},信息增益为：{c_info_gain}')\n",
    "    best_ = max(features, key=lambda x: x[1])\n",
    "    print(f'最大信息增益的特征为：{dataset[\"feature\"][best_[0]]}, \\\n",
    "信息增益为：{best_[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前特征：年龄,信息增益为：0.08\n",
      "当前特征：有工作,信息增益为：0.32\n",
      "当前特征：有自己的房子,信息增益为：0.42\n",
      "当前特征：信贷情况,信息增益为：0.36\n",
      "最大信息增益的特征为：有自己的房子, 信息增益为：0.42\n"
     ]
    }
   ],
   "source": [
    "train_info_gain(createdata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 信息增益比（率）\n",
    "- 定义：特征A对训练数据集D的信息增益比$gR(D,A)$为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)之比$，即：\n",
    "$$gR(D,A)=\\frac{g(D,A)}{H_A(D)}$$\n",
    "其中$H_A(D) = - \\sum_{i=1}^n \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算上例中每个特征的信息增益比\n",
    "def feature_entropy(data: np.array) -> list:\n",
    "    \"\"\" 计算数据集中每个特征的经验熵 \n",
    "    args\n",
    "        data: feature data not incloud labels\n",
    "    \"\"\"\n",
    "    rows, cols = data.shape\n",
    "    ent_list = []\n",
    "    for i in range(cols):\n",
    "        feature_count = Counter(data[:, i])\n",
    "        ent = -sum(\n",
    "            [(v / rows) * np.log2(v / rows) for v in feature_count.values()])\n",
    "        ent_list.append(ent)\n",
    "        \n",
    "    return ent_list\n",
    "\n",
    "def info_gain_rate(datasets) -> tuple:\n",
    "    \"\"\" 计算信息增益比 \"\"\"\n",
    "    rows, cols = datasets.shape\n",
    "    cols -= 1\n",
    "    \n",
    "    ent = empirical_entropy(datasets)    # 经验熵\n",
    "    feature_ent = feature_entropy(datasets[:, :-1])\n",
    "    # print(feature_ent)\n",
    "    # 计算信息增益\n",
    "    gain_list = []\n",
    "    for i in range(cols):\n",
    "        cond_ent = cond_empirical_entropy(datasets, i)\n",
    "        gain_list.append(ent - cond_ent)\n",
    "    \n",
    "    # print(gain_list)\n",
    "    # 计算信息增益比\n",
    "    return list(np.array(gain_list) / np.array(feature_ent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中信息增益熵最大的特征是:有自己的房子\n"
     ]
    }
   ],
   "source": [
    "data = createdata()\n",
    "gain_rates = info_gain_rate(data['data'])\n",
    "max_idx = gain_rates.index(max(gain_rates))\n",
    "print(f'数据中信息增益熵最大的特征是:{data[\"feature\"][max_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征：年龄,的信息增益比为：0.05237190142858302\n",
      "特征：有工作,的信息增益比为：0.3524465495205019\n",
      "特征：有自己的房子,的信息增益比为：0.4325380677663126\n",
      "特征：信贷情况,的信息增益比为：0.23185388128724224\n"
     ]
    }
   ],
   "source": [
    "for idx, value in enumerate(gain_rates):\n",
    "    print(f'特征：{data[\"feature\"][idx]},的信息增益比为：{value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## ID3算法\n",
    "- 核心：在决策树熵各个节点上应用信息增益准则选择特征，递归的构建决策树。\n",
    "- 方法：从根节点开始，对节点计算所有可能特征的信息增益，选择信息增益最大的特征作为节点特征，由该特征的不同取值构建子节点，再对子节点递归调用上述方法。\n",
    "#### 构建过程\n",
    "- 输入：训练数据集D，特征集A 阈值$\\epsilon$\n",
    "- 输出：决策树 T。\n",
    "1. 若D中所有的实例属于同一类$C_k$,则T为单节点树，并将类$C_k$作为改节点的类标记，返回T；\n",
    "2. 若$A = \\emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点类标记，返回$T$；\n",
    "3. 否则，按之前的算法计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；\n",
    "4. 如果$A_g$的信息增益小于阈值$\\epsilon$,则$T$为单节点树，将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$；\n",
    "5. 否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$将D划分为若干非空子集$D_i$,将$D_i$中实例数最大的作为类标记，构建子节点，由节点及其子节点构成树T，返回$T_i$；\n",
    "6. 对第$i$个子节点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归调用步骤1-5，得到树$T_i$,返回T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义树类\n",
    "class Node:\n",
    "    def __init__(self, root=True, label=None, feature_name=None, feature=None):\n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.feature_name = feature_name\n",
    "        self.feature = feature\n",
    "        self.tree = {}\n",
    "        self.result = {\n",
    "            'label': self.label,\n",
    "            'feature': self.feature,\n",
    "            'tree': self.tree \n",
    "        }\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{}'.format(self.result)\n",
    "    \n",
    "    def add_node(self, value, node):\n",
    "        self.tree[value] = node\n",
    "        \n",
    "    def predict(self, features):\n",
    "        if self.root is True:\n",
    "            return self.label\n",
    "        return self.tree[features[self.feature]].predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义决策树\n",
    "class DTree:\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self._tree = {}\n",
    "    \n",
    "    # 计算经验熵\n",
    "    @staticmethod\n",
    "    def calc_ent(data: np.array) -> float:\n",
    "        \"\"\" 计算熵，熵仅与信息分布有关，与其他信息无关\n",
    "        data: 求一组信息的信息熵\n",
    "        \"\"\"\n",
    "        data_len = data.shape[0]\n",
    "        labels_count = Counter(data)    # 统计类别结果\n",
    "        ent = -sum([\n",
    "            (c / data_len) * np.log2(c / data_len) for c in labels_count.values()])\n",
    "        return ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def cond_ent(x_data, labels):\n",
    "        \"\"\" 计算条件经验熵 \n",
    "        x_data: 训练数据集的一个特征\n",
    "        y: 训练数据集对应的类标签\n",
    "        \"\"\"\n",
    "        assert len(x_data) == len(labels)\n",
    "        length = len(x_data)\n",
    "        feature_count = Counter(x_data)     # 统计每个特征的数据\n",
    "        \n",
    "        # 计算当前特征取每个值时的数据集的经验熵H(D_i)\n",
    "        feature_ent = [\n",
    "            DTree.calc_ent(labels[x_data == f]) for f in feature_count.keys()]   \n",
    "        # 计算该特征每个取值的频率len(D_i) / len(D)\n",
    "        feature_proba = [c / length for c in feature_count.values()]\n",
    "\n",
    "        return sum([a * b for a, b in zip(feature_ent, feature_proba)])\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_gain(ent, cond_ent):\n",
    "        return ent - cond_ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_gain_train(x_train, y_train):\n",
    "        \"\"\" 计算数据集的信息增益，并找到信息增益最大的特征 \n",
    "        return: best_feature, best_info_gain\n",
    "        \"\"\"\n",
    "        assert len(x_train) == len(y_train)\n",
    "        cols = x_train.shape[1]\n",
    "        \n",
    "        ent = DTree.calc_ent(y_train)\n",
    "        cond_ents = [DTree.cond_ent(x_train[:, idx], y_train) for idx in range(cols)]\n",
    "        gains = [DTree.info_gain(ent, cond) for cond in cond_ents]\n",
    "\n",
    "        best_gain = max(gains)\n",
    "        return gains.index(best_gain), best_gain\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        assert len(x_train) == len(y_train)\n",
    "        features = np.array([i for i in range(x_train.shape[1])])\n",
    "        \n",
    "        # 1.若数据集中的所有数据都同属于一类，则T为但节点树，此类作为该节点的标记，返回树\n",
    "        if len(np.unique(y_train)) == 1:\n",
    "            return Node(root=True, label=y_train[0])\n",
    "\n",
    "        # 如果特征不为空,则T为单节点树，此实例中类别数量最多作为该节点标记\n",
    "        if len(features) == 0:\n",
    "            # 找到数据集中类别数量最多的分类\n",
    "            max_label = sorted(\n",
    "                Counter(y_train).items(), reverse=True, key=lambda x: x[1])[0][0]\n",
    "            return Node(root=True, label=max_label)\n",
    "        \n",
    "        # 3.计算最大信息增益的特征\n",
    "        max_feature, max_info_gain = DTree.info_gain_train(x_train, y_train)\n",
    "        max_feature_name = features[max_feature]\n",
    "        \n",
    "        # 4.若信息增益小于阈值，则置T为单节点树，样本实例中类别树最多的作为当前节点的标签\n",
    "        if max_info_gain < self.epsilon:\n",
    "            max_label = sorted(\n",
    "                Counter(y_train).items(), reverse=True, key=lambda x: x[1])[0][0]\n",
    "            return Node(root=True, label=max_label)\n",
    "        \n",
    "        # 5. 构建Ag子节点，按照特征的所有可能取值划分数据集，每个数据集的标签为其类别最多的类别\n",
    "        node_tree = Node(\n",
    "            root=False, feature_name=max_feature_name, feature=max_feature)\n",
    "        # 获取Ag特征的所有取值\n",
    "        feature_list = Counter(x_train[:, max_feature]).keys()\n",
    "        # 6.移除信息增益最大的特征，并递归生成子节点\n",
    "        for f in feature_list:\n",
    "            drop_feature_index = x_train[:, max_feature_name] == f\n",
    "            \n",
    "            sub_x_train = np.delete(\n",
    "                x_train[drop_feature_index], max_feature_name, axis=1)\n",
    "            sub_y_train = y_train[drop_feature_index]\n",
    "            \n",
    "            sub_tree = self.train(sub_x_train, sub_y_train)\n",
    "            node_tree.add_node(f, sub_tree)\n",
    "        \n",
    "        return node_tree\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        self._tree = self.train(x_train, y_train)\n",
    "        return self._tree\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        return self._tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "x_data = createdata()['data'][:, :-1]\n",
    "y_data = createdata()['data'][:, -1]\n",
    "td = DTree()\n",
    "dt_tree = td.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'是'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.predict(x_data[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5算法，基于信息增益比，只需要将上例中计算信息增益部分替换为信息增益比即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART算计，基于基尼系数\n",
    "- 基尼指数：分类问题中，假设有K个类，样本点属于第$k$类的概率为$p_k$,则概率分布的基尼系数定义为\n",
    "$$Gini(p)=\\sum_{k=1}^K p_k(1-p_k) = 1 - \\sum_{k=1}^K p_k^2$$\n",
    "- 对于给定的样本集合D，其基尼指数为\n",
    "$$Gini(D) = 1 - \\sum_{k=1}^K \\left(\\frac{|C_k|}{|D|} \\right)^2$$\n",
    "- 在特征A的条件下，集合D的基尼指数定义为\n",
    "$$Gini(D, A)=\\frac {|D_1|}{|D|} Gini(D_1) + \\frac{|D_2|}{|D|} Gini(D_2)$$\n",
    "- 基尼系数$Gini(D)$,表示集合D的不确定性，基尼指数$Gini(D, A)$表述经A=a分割后集合D的不确定性，基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树剪枝\n",
    "### 预剪枝\n",
    "- 在构造决策树的同时进行剪枝。\n",
    "- 在构造决策树时，一般情况下，当数据集的信息熵无法继续降低时，即停止创建决策树的分支。在预剪枝中，设定一个阈值，当决策树的信息增益小于这个阈值时，即停止剪枝。\n",
    "\n",
    "### 后剪枝\n",
    "- 决策树构建完成后进行剪枝，\n",
    "- 剪枝过程中对拥有同样父节点的一组节点进行检查，判断如果将其合并熵的增加量是否小于某一阈值，如果确实小，则这一组节点可以合并为一个节点\n",
    "\n",
    "### 后剪枝算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x_train, y_train):\n",
    "    \"\"\" 计算数据集的基尼系数\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
