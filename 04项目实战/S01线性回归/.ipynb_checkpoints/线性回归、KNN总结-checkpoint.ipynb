{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归\n",
    "    1、目标函数：\n",
    "    在算法模型优化的过程中，优化的方向函数--》每次迭代都让这个目标函数值最小化--》最优解取目标函数值最小化时对应的参数值；\n",
    "    2、损失函数：\n",
    "    一般情况下和目标函数是同一个，有时损失函数指我们模型参数给定的时候，预测值和实际值之间的差距值\n",
    "    3、目标函数优化方式：\n",
    "    目标函数一般是凸函数，常用的 优化方案：最小二乘法、梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归+多项式扩展：\n",
    "    1、如果样本本身不具有线性关系，那么进行预测就会导致模型欠拟合；\n",
    "    2、样本数据在低纬度空间中不具有线性关系，但是映射到高纬度空间中的时候，数据就可能变成线性关系，从而使用线性回归进行预测\n",
    "    3、如果映射维度特别高，那么数据会完全变成线性关系从而导致模型在训练集合上效果非常好，但是在测试集上表现不好，即过拟合；（训练数据集特别契合算法模型，但实际数据一定存在和训练数据集不一致的地方，导致在其他数据集上预测效果不佳）\n",
    "    过拟合解决方案：L1-norm, l2-norm(用的较多）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic回归\n",
    "    1、二分类算法，计算样本属于一个某一个类别的概率为1-p，那么属于另一个类别的概率为1-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax 回归\n",
    "    1、多分类算法，需要计算样本属于某个类别的概率，最终认为样本属于概率值最大的那个类别；\n",
    "    2、softmax 会为每个参数类别训练一个theta值，sotamax需要求解的参数，是一个由k个theta向量组成的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
