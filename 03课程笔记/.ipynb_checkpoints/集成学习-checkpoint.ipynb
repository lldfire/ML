{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 思想：将若干个学习器组合之后产生一个新的学习器，将若分类器的效果变得更好；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见的继承学习思想：\n",
    "1. Bagging:随机森林\n",
    "2. Boosting:\n",
    "3. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么使用集成学习\n",
    "    1、若分类器间存在一定的差异性，导致分类的边界不同，将多个弱分类器合并后，可以得到更合理的边界，实现更好的效果；\n",
    "    2、\n",
    "    3、\n",
    "    4、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging ：自举汇聚法，在原始的数据集上通过有放回的抽样方式，重新选择出S个新的数据集来分别训练S个分类器的集成技术。在预测新样本的时候会使用多数投票或求均值的方式来统计最终分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting：提升学习，可以用于回归和分类问题中，每一步产生弱预测模型加权累加到总模型中，常见的模型有：\n",
    "- Adaboost\n",
    "- Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost:是一种迭代算法，每轮迭代产生一个学习器，使用学习器预测所有样本，根据预测结果，对样本不同的权重，预测正确，降低权重，预测错误，提高权重。直到模型错误率小于某一值或达到一定的迭代次数为止。\n",
    "- 样本加权\n",
    "---\n",
    "#### Grasient boosting(GBDT):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代决策树和随机森林的区别：\n",
    "- 随机森林抽取不同的样本构建不同的决策树，每棵决策树之间是没有关系的\n",
    "- 迭代决策树在构建子树的时候，使用之前树构建结果后形成的残差作为输入数据构建下一个子树，最终预测按照子树的构建顺序进行预测，并将预测结果叠加。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
