{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回顾\n",
    "#### 最大似然估计（MLE）\n",
    "- 利用已知样本反推最有可能导致这样结果的参数值的计算过程。换言之，就是知道数据是从某种分布中提取出来的，但不知道这个分布的具体参数值，最大似然估计就是用来估计模型的参数。最大似然估计的目标就是找出一组参数，使得模型产生的观测数据的概率值最大。\n",
    "- MLE求解过程：\n",
    "    - 编写似然函数（即联合概率函数）<似然函数，在样本固定的情况下，样本出现的概率与参数$\\theta$之间的函数\n",
    "    - 对似然函数求对数并整理\n",
    "    - 求导数\n",
    "    - 求解似然函数方程，得到概率值\n",
    "\n",
    "#### 贝叶斯算法估计\n",
    "- 根据先验概率和样本的分布情况来计算后验概率的一种方式\n",
    "- 贝叶斯算法求解过程：\n",
    "    - 求解先验概率\n",
    "    - 寻找样本的分布情况，常见：高斯分布、多项式分布、伯努利分布等\n",
    "    - 根据样本的分布情况计算样本的似然度\n",
    "    - 先验概率 * 似然度即为 再除以样本发生的全概率即为后验概率\n",
    "    \n",
    "#### 最大后验概率估计（MAP）\n",
    "- 贝叶斯算法的转换，不考虑调整因子的分母\n",
    "\n",
    "#### K-means 算法\n",
    "- K-均值聚类算法\n",
    "- 步骤：\n",
    "    - 选择初始的 k 个簇心点$\\mu_1, \\mu_2, ···, \\mu_k$\n",
    "    - 将样本$X_i$标记为距离簇中心最近的簇\n",
    "    - 迭代处理所有的簇，计算每个样本点对应的簇\n",
    "    - 更新簇中心\n",
    "    - 重复上述，直到算法收敛\n",
    "\n",
    "- 算法收敛条件\n",
    "    - 迭代次数、簇中心变化率、MSE、MAE \n",
    "\n",
    "\n",
    "### EM算法（最大期望算法）\n",
    "- 一种迭代类型的算法，是一种在概率模型中寻找最大似然估计或者最大后验概率参数的算法，其概率模型 依赖于无法观测的隐藏变量\n",
    "- EM算法流程\n",
    "     - 初始化分布参数\n",
    "     - 重复下面两步，直到模型收敛\n",
    "         - E步骤：估计隐藏变量的概率分布期望函数\n",
    "         - M步骤：根据期望函数重新估计分布参数\n",
    "\n",
    "- 样本数据$x={x_1,x_2,···x_n}$,联合分布概率$p(x,z;\\theta)$, 条件分布概率$p(z|x;\\theta),$最大迭代次数J\n",
    "    - 随机初始化模型参数$\\theta$的初始值$\\theta^0$\n",
    "    - 开始EM算法的迭代处理：\n",
    "        - E步：计算联合分布的条件概率期望值：\n",
    "        $$Q^j=p\\left(z|x;\\theta^j\\right)$$\n",
    "        $$l(\\theta)=\\sum_{j=1}^m \\sum_z Q^j \\log\\left(p(x,z;\\theta^0)\\right)$$\n",
    "        - M步：极大化$l(\\theta)$函数，得到$\\theta^{j+1}$:\n",
    "        $$\\theta^{j+1} = argmaxl(\\theta)$$\n",
    "        - 如果$\\theta{j+1}$已经收敛则输出模型参数$\\theta$\n",
    "        \n",
    "#### EM 算法收敛证明\n",
    "- EM 算法收敛证明，只要能证明对数似然函数的值在迭代过程中是增加的即可：\n",
    "$$\\sum_{i=1}^m \\log \\left( p(x^j; \\theta^{j+1})\\right) \\geq \\sum_{i=1}^m \\log \\left( p(x^j; \\theta^j)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯混合模型（GMM）\n",
    "- 该算法由多个高斯模型线性叠加混合而成。每个高斯模型被称之为 component。GMM算法描述的是数据本身存在的一种分布方式。\n",
    "- GMM算法常用于聚类算法中，component 的数量就是簇的数量\n",
    "- 假定GMM算法有 K 个gaussian分布线性叠加而成，那么其概率密度函数如下：\n",
    "$$p(x) = \\sum_{k=1}^K \\pi_k p(x;\\mu_k, \\Sigma_k)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
