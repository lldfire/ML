{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息熵\n",
    "   高信息熵，随机变量X是均匀分布的，各种取值情况都是等概率出现的\n",
    "   低信息熵，随机变量X是不均匀分布的，取值可能大也可能小。\n",
    "信息熵越低越有价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件熵\n",
    "    给定条件X的情况下，所有不同x值的情况下随机变量Y的信息熵的平均值就叫条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树\n",
    "    已知各种情况发生的概率的情况下，通过构建决策树来进行分析的一种方式，是一种预测模型，代表的是对象属性和对象值之间的映射关系；决策树是一种树形结构。是一种常用的有监督的分类算法\n",
    "    决策树分为两大类：分类树和回归树，前者用于分类标签值，后者用于预测连续值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树构建过程：\n",
    "    重点：选择分裂属性，让一个分裂子类中的分类尽可能的属于一个类别\n",
    "    1、将所有的特征值看成一个个节点；\n",
    "    2、遍历每一种分割方式，找到最好的分割点；将数据划分为不同的子集，计算划分之后所有子节点的 纯度；\n",
    "    3、对上一步产生的分割，选择出最优特征及最优的划分方式，得出最终子节点；\n",
    "    4、对子节点继续执行2、3步，直到每个子节点足够的纯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树特征属性类型，根据特征属性的不同类型，在构建决策树时，采用不同的方式：\n",
    "    1、属性是离散值，且不要求生成二叉决策树，此时一个属性就是一个分支\n",
    "    2、属性是离散值，且要求生成二叉决策树，此时用属性划分的子集进行测试，按照属于此子集和不属于此子集两种分支\n",
    "    3、属性是连续值，可以确定一个值作为分裂点split_point,按照>split_point和小于等于split_point分割两个分支"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树量化纯度，判断数据集的纯度可以通过三个公示进行判断：\n",
    "    1、熵：信息熵越小，表示数据越纯\n",
    "    2、gini系数：值越小数据越纯\n",
    "    3、错误率：值越小数据越纯\n",
    "当计算出各个特征属性的量化纯度后，使用信息增益率来选择当前数据集的分割特征属性，如果信息增益率值越大，表示在该特征属性上损失的纯度越大，就应该在决策树上册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树停止条件\n",
    "    1、每个子节点只有一种类型的时候，停止构建\n",
    "    2、当前节点中记录数小于某个阈值，同时迭代次数达到给定值；\n",
    "方式一可能会导致过拟合，常用方式二作为停止条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树生成算法 \n",
    "    1、ID3算法：内部使用信息熵和信息增益来进行构建，每次迭代选择信息增益最大的属性作为分割属性。\n",
    "        不支持连续变量，划分属性取值必须是离散的\n",
    "        对缺省值敏感，需要手动填充空值\n",
    "         某个属性划分完子集之后，不能再继续使用该属性划分\n",
    "    2、C4.5算法：特别经典的一种决策树算法，使用信息增益率来取代ID3中的信息增益，在树的构建构成中会进行剪枝\n",
    "        选择信息增益率最大的属性\n",
    "    3、CART算法：使用基尼系数最为数据纯度量化指标，选择基尼增益作为分割属性的选择标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树优化策略\n",
    "    1、剪枝：最基本，最有用的一种优化方案；\n",
    "    2、Random Forst\n",
    "\n",
    "#### 决策树的剪枝\n",
    "    1、前置剪枝：构建决策树的过程中，提前停止，此法做出来的效果比较差。\n",
    "    2、后置剪枝：决策树构建好之后开始剪枝，一般有两种方式：1）用单一子节点代替整个树，叶节点的分类采用树中最主要的分类；2）将一个子树完全替代另一个子树。这种剪枝方法效率比较低，但是效果比较好。\n",
    "\n",
    "#### 决策树剪枝过程\n",
    "    1、计算所有非叶子节点的剪枝系数\n",
    "    2、查找最小剪枝系数的节点，将其子节点删除，得到决策树；如果存在多个最小剪枝系数，选择包含数据项最多的剪枝系数进行剪枝操作。\n",
    "    3、重复上述操作得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  回归树\n",
    "    回归树预测结果为叶子节点中所有可能值的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类树和回归树的区别\n",
    "    1、分类树中使用的是信息熵、gini系数、错误率来度量信息的纯度；回归树使用MSE、MAE作为度量纯度指标；\n",
    "    2、分类树使用叶子节点包含最多的那个类别为当前节点的预测值；回归树使用包含叶子节点中所有的样本的目标属性作为预测值；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
